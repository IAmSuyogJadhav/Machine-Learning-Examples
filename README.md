# Machine-Learning-Examples

- Following the tutorials from https://github.com/aymericdamien/TensorFlow-Examples
- Reading material from http://neuralnetworksanddeeplearning.com/

---
## Implementation:
Implement the following algorithms with a basic knowledge of Data Analytics and Machine Learning

1. Linear Regression
2. Logistic Regression
3. kNN
4. Random Forests: _Needs a fix_
5. CNN: _Also read the awesome tutorial on backpropogation from the same book (Chap 2) and why it works so well for neural nets_
6. CNN with Abstraction using Tensorflow.: _Needs a fix. Also read what happens when the network has a slow learning rate due to the L2 cost function from chap 3 of the above book_ Implement using: 
6(a) CNN with Abstraction using Tensorflow (using Cross Entropy)
6(b) CNN with Abstraction using Tensorflow (using SoftMax)

7. Convolution Neural Network: _read chap 4_

8. Cat vs Dog classifier: _Tried an example using a vanilla CNN._

9. TensorBoard-Working: _Example code for using tensorboard._

10. RNN(LSTM): _The timesteps are added to the LSTM cell. Read [this](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) blog for more info on them_

- **Saving and Restoring a Model**: Excellent blog post [here] (http://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/)

## Regularization
Read about Regularization and why it works. Simple is better, but not neccesarily.

1. Over Fitting
2. No Free Lunch Theorem
---
- L1 Regularization - Makes the network smaller with lesser number of connections.
- L2 Regularization - Makes sure the weights are not too big.
- Dropout - Works similar to averaging multiple nets.
- Data Augmentation - Makes the network more susceptible to changes

-----------------------------------------------------------------------------------------------------------------------------------------

Special Thanks to [Naresh](https://naresh1318.github.io/) for his awesome backpropogation for my errors!
